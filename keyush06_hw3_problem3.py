# -*- coding: utf-8 -*-
"""keyush06_hw3_problem3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Tu5Lh6xy_XjFNZYAm98ZQAGQ7twaKEtR
"""

#%%
import numpy as np
import math
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import matplotlib.pyplot as plt
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

files = ["leo.txt","Shakespeare.txt","The study of magic and religion.txt"]
all_text = "all_text" #SAVING THE FILE
con_text = ""

for name in files:
    with open(name, 'r',encoding='utf-8') as file:

        file_content = file.read()[5:]
        con_text += file_content
        # con_text = con_text[4:]
        # print(name)

with open(all_text, 'w',encoding='utf-8') as output:
    output.write(con_text)

## Splitting into train and validation data
train = con_text[:int(len(con_text)*0.75)]
val = con_text[int(len(con_text)*0.75):]
train_trans = train
val_trans = val

def seq_making(data, length):
    l1, l2 = [], []
    i = 0
    while i + length+1 <= len(data):
        x = data[i:i+length]
        y = data[i+1:i+length+1]
        l1.append(x)
        l2.append(y)
        i = i+length
    return l1, l2

seq_data, seq_op = seq_making(train, length=32)
seq_data_val, seq_op_val = seq_making(val, length=32)

def vocab(text):
    unique_chars = set(text)
    size_vocab = len(unique_chars)

    mapping = {char:index for index,char in enumerate(unique_chars)}

    return size_vocab,mapping

#------------------------------------------------------------------------------

def one_hot_encode(data, char_to_index, size_vocab):
    one_hot = np.zeros((len(data), size_vocab), dtype=int)

    # Convert the sequence into one-hot encoding
    for i, char in enumerate(data):
        char_index = char_to_index[char]
        one_hot[i, char_index] = 1

    return np.array(one_hot)

def batchify(seq,batch_size):
    batches = []

    for i in range(0,len(seq),batch_size):
        b = seq[i:i+batch_size]

        if len(b)==batch_size:
            batches.append(b)

    return batches

## Encoding the data
size_vocab,mapping = vocab(con_text)
size_vocab_train,mapping_train_data = vocab(train)
size_vocab_val,mapping_val_data = vocab(val)
mapping_v2 = {value: key for key, value in mapping.items()}

## Dividing into batches of 64
batch_data, batch_op = batchify(seq_data,64), batchify(seq_op,64)
test_batch_data, test_batch_op = batchify(seq_data_val,64), batchify(seq_op_val,64)
batch_data = np.array(batch_data)
batch_op = np.array(batch_op)
test_batch_data = np.array(test_batch_data)
test_batch_op = np.array(test_batch_op)

class RNNModel(nn.Module):

    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):
        super(RNNModel, self).__init__()
        self.ntoken = ntoken
        self.drop = nn.Dropout(dropout)
        # self.encoder = nn.Embedding(ntoken, ninp)
        self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity="tanh", dropout=dropout,batch_first = True)
        self.decoder = nn.Linear(nhid, ntoken)


        self.init_weights()

        #self.rnn_type = rnn_type
        self.nhid = nhid
        self.nlayers = nlayers

    def init_weights(self):
        initrange = 0.1
        # nn.init.uniform_(self.encoder.weight, -initrange, initrange)
        nn.init.zeros_(self.decoder.bias)
        nn.init.uniform_(self.decoder.weight, -initrange, initrange)

    def forward(self, input, hidden):
        emb = self.drop(input)
        output, hidden = self.rnn(emb, hidden)
        output = self.drop(output)
        decoded = self.decoder(output)
        decoded = decoded.view(-1, self.ntoken)
        return F.log_softmax(decoded, dim=1), hidden

    def init_hidden(self, bsz):
        weight = next(self.parameters())
        return weight.new_zeros(self.nlayers, bsz, self.nhid)

def train(net, optimizer, criterion, batch_data, batch_op, test_batch_data, test_batch_op, batch_size,
          size_vocab,mapping,epochs, model_name, plot):

    model = net.to(device)
    total_step = batch_data.shape[0]
    overall_step = 0
    train_loss_values = []
    train_error = []
    train_error_weights = []
    val_loss_values = []
    val_error = []
    weight_updates = 0
    for epoch in range(epochs):
        correct = 0
        total = 0
        running_loss = 0.0
        hidden = model.init_hidden(batch_size)

        start_time = time.time()
        # print(batch_data[0])
        for batch in range(batch_data.shape[0]):
            data= batch_data[batch]
            targets = batch_op[batch]
            # print(data.shape)
            # print(targets.shape)

            # for i in data[0]:
            #   print(i)
            data_lst,targets_lst = [],[]
            for x in data:
              one=one_hot_encode(x,mapping,size_vocab)
              data_lst.append(one)

            for y in targets:
              one_tgt = one_hot_encode(y,mapping,size_vocab)
              targets_lst.append(one_tgt)

            #batch_size = 16
            # print(data_lst[0])

            data=torch.tensor(data_lst, dtype = torch.float).view(batch_size,32,size_vocab)
            # print(data.shape)
            targets=torch.tensor(targets_lst,dtype=torch.float).view(batch_size,32,size_vocab)
            # print(targets.shape)
            # print(data[1])
            ## The above function has mapping related to the whole data and size vocab is added as it is the
            # length of unique characsters and our data might have any of them
            data=data.to(device)
            targets=targets.to(device)
            # targets = targets.view(-1, size_vocab)
            # print(data[0])

            model.zero_grad()

            if model_name == 'Transformer':
                output = model(data)
                output = output.view(-1, size_vocab)
            else:
                hidden = hidden.detach()
                output, hidden = model(data, hidden)
                # print('***')
                # print(output.shape)


            _, labels = torch.max(targets, 2)
            # print(labels.shape)
            labels=labels.view(-1)
            loss = criterion(output.view(-1, size_vocab), labels.view(-1))
            # loss = criterion(output, labels)

            loss.backward()

            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)



            running_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            optimizer.step()
            weight_updates+=1
            if (batch+1) % 32 == 0:
              print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, batch+1, total_step, loss.item()))
            if plot:
              info = { ('loss_' + model_name): loss.item() }

            train_error_weights.append(100-100*correct/total)

            ## Considering the validation error for future 32 characters after every 1000 weight updates
            if weight_updates%1000 == 0:
                model.eval()
                running_loss_error_test=0
                test_total=0
                test_correct=0
                hidden_op = model.init_hidden(batch_size)
                with torch.no_grad():
                    correct_val = 0
                    total_val = 0
                    for i in range(0, test_batch_data.shape[0]-800):
                        test_data, test_target = test_batch_data[i],test_batch_op[i]

                        #Performing one-hot encoding
                        test_data_lst,test_target_lst = [],[]
                        for x in test_data:
                          one=one_hot_encode(x,mapping,size_vocab)
                          test_data_lst.append(one)

                        for y in test_target:
                          one_tgt = one_hot_encode(y,mapping,size_vocab)
                          test_target_lst.append(one_tgt)

                        # batch_size = 16

                        test_data=torch.tensor(test_data_lst, dtype=torch.float).view(batch_size,32,size_vocab)
                        test_target=torch.tensor(test_target_lst,dtype=torch.float).view(batch_size,32,size_vocab)

                        # test_data=torch.tensor(one_hot_encode(test_data,mapping,size_vocab),dtype=torch.float)
                        # test_target=torch.tensor(one_hot_encode(test_target,mapping,size_vocab),dtype=torch.float)

                        test_data=test_data.to(device)
                        test_target=test_target.to(device)
                        # test_target = test_target.view(-1, size_vocab)
                        if model_name == 'Transformer':
                            test_op = model(test_data)
                            test_op = test_op.view(-1, size_vocab)
                        else:
                            test_op, hidden_op = model(test_data, hidden_op)
                            hidden_op = hidden_op.detach()

                        _, test_labels = torch.max(test_target, 2)
                        test_labels=test_labels.view(-1)
                        loss_test = criterion(test_op, test_labels)
                        running_loss_error_test += (test_data.shape[0])*loss_test.item()
                        _, test_predicted = torch.max(test_op.data, 1)
                        test_total += test_labels.size(0)

                        test_correct += (test_predicted == test_labels).sum().item()
                model.train()
                print('Accuracy of the network on the test data: {} %'.format(100 * test_correct / test_total))
                val_error.append(100-100*test_correct/test_total)
                val_loss_values.append(running_loss_error_test/((test_batch_data.shape[0])))

        train_loss_values.append(running_loss/(batch_data.shape[0]))
        train_error.append(100-100*correct/total)
        # print(train_loss_values)

    return val_error,val_loss_values,train_error,train_loss_values, train_error_weights



ntokens = size_vocab
nhid = 150
lr = 0.001
nlayers = 1
clip = 0.5
dropout = 0.0
ninp = size_vocab
model_rnn = RNNModel(ntokens, ninp, nhid, nlayers, dropout).to(device)
criterion = nn.NLLLoss()
epochs = 3
optimizer = optim.Adam(model_rnn.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, amsgrad=False)
val_error,val_loss_values,train_error,train_loss_values, train_error_weights= train(model_rnn, optimizer, criterion, batch_data, batch_op, test_batch_data, test_batch_op, 64, size_vocab, mapping, epochs,'RNN curve','RNN')
torch.save(model_rnn.state_dict(), "model_params.pth")

"""**Due to GPU constraints I could not run the code again, however, we have the loss under permissible range as discussed (between 1.7 - 1.9). Kindly note that this loss reduces a bit more, but it gives us a general idea of converging.**"""

plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(train_loss_values)
plt.title('Training Loss')
plt.xlabel('Number of Epochs')
plt.ylabel('Loss')

# Plotting the training error.
# plt.figure(figsize=(5, 5))
plt.subplot(1, 2, 2)
plt.plot(val_loss_values)
plt.title('Validation loss')
plt.xlabel('Number of Weight Updates')
plt.ylabel('Loss')

plt.tight_layout()
plt.show()


#%%

plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(train_error)
plt.title('Training Error')
plt.xlabel('Number of Epochs')
plt.ylabel('Error')

# Plotting the training error.
# plt.figure(figsize=(5, 5))
plt.subplot(1, 2, 2)
plt.plot(val_error)
plt.title('Validation Error')
plt.xlabel('Number of Weight Updates')
plt.ylabel('Error')

plt.tight_layout()
plt.show()

# plt.subplot(1, 2, 2)
plt.figure(figsize=(15, 5))
plt.plot(train_error_weights)
plt.title('Training error')
plt.xlabel('Number of Weight Updates')
plt.ylabel('Loss')

plt.tight_layout()
plt.show()

"""# **Generation**"""

sequence_length = 32
chars = ['k','e','y','u','s','h']
# hid = model_rnn.init_hidden(1)

for x in chars:
  hid = model_rnn.init_hidden(1)
  # print(hid.shape)
  sentence = x
  for i in range(sequence_length):
    data_one_rnn = torch.tensor(one_hot_encode(x,mapping,size_vocab),dtype=torch.float).to(device).unsqueeze(0)
    # print(data_one_rnn.shape)
    op,hid = model_rnn(data_one_rnn,hid)
    _, predicted = torch.max(op, dim=1)
    char_next = mapping_v2[predicted.item()]
    sentence+=char_next
    data_one_rnn = torch.tensor([[one_hot_encode(char_next,mapping,size_vocab)]],dtype=torch.float).to(device).unsqueeze(0)

  print(sentence)

"""## **Data Creation for Transformers**"""

#### Make data for transformers

# making sequence
seq_data_trans, seq_op_trans = seq_making(train_trans, length=128)
seq_data_val_trans, seq_op_val_trans = seq_making(val_trans, length=128)

## Encoding the data
# size_vocab_trans,mapping_ = vocab(con_text)
# size_vocab_train,mapping_train_data = vocab(train)
# size_vocab_val,mapping_val_data = vocab(val)

## Dividing into batches of 64
batch_data_trans, batch_op_trans = batchify(seq_data_trans,256), batchify(seq_op_trans,256)
test_batch_data_trans, test_batch_op_trans = batchify(seq_data_val_trans,256), batchify(seq_op_val_trans,256)
batch_data_trans = np.array(batch_data_trans)
batch_op_trans = np.array(batch_op_trans)
test_batch_data_trans = np.array(test_batch_data_trans)
test_batch_op_trans = np.array(test_batch_op_trans)

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

class TransformerModel(nn.Transformer):
    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):
        super(TransformerModel, self).__init__(d_model=ninp, nhead=nhead, dim_feedforward=nhid, num_encoder_layers=nlayers)
        self.model_type = 'Transformer'
        self.src_mask = None
        self.pos_encoder = PositionalEncoding(ninp, dropout)

        #self.input_emb = nn.Embedding(ntoken, ninp)
        self.ninp = ninp
        self.decoder = nn.Linear(ninp, ntoken)

        self.init_weights()

    def _generate_square_subsequent_mask(self, sz):
        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
        return mask

    def init_weights(self):
        initrange = 0.1
        #nn.init.uniform_(self.input_emb.weight, -initrange, initrange)
        nn.init.zeros_(self.decoder.bias)
        nn.init.uniform_(self.decoder.weight, -initrange, initrange)

    def forward(self, src, has_mask=True):
        if has_mask:
            device = src.device
            if self.src_mask is None or self.src_mask.size(0) != len(src):
                mask = self._generate_square_subsequent_mask(len(src)).to(device)
                self.src_mask = mask
        else:
            self.src_mask = None

        #src = self.input_emb(src) * math.sqrt(self.ninp)
        src = self.pos_encoder(src)
        output = self.encoder(src, mask=self.src_mask)
        output = self.decoder(output)
        return F.log_softmax(output, dim=-1)

def train(net, optimizer, criterion, batch_data, batch_op, test_batch_data, test_batch_op, batch_size,
          size_vocab,mapping,epochs, model_name, plot):

    model = net.to(device)
    total_step = batch_data.shape[0]
    overall_step = 0
    train_loss_values = []
    train_error = []
    val_loss_values = []
    val_error = []
    weight_updates = 0
    for epoch in range(epochs):
        correct = 0
        total = 0
        running_loss = 0.0

        start_time = time.time()


        # print(batch_data[0])
        for batch in range(batch_data.shape[0]):
            data= batch_data_trans[batch]
            targets = batch_op_trans[batch]
            # print(data.shape)
            # print(targets.shape)

            # for i in data[0]:
            #   print(i)
            data_lst,targets_lst = [],[]
            for x in data:
              one=one_hot_encode(x,mapping,size_vocab)
              data_lst.append(one)

            for y in targets:
              one_tgt = one_hot_encode(y,mapping,size_vocab)
              targets_lst.append(one_tgt)

            #batch_size = 16
            # print(data_lst[0])

            data=torch.tensor(data_lst, dtype = torch.float).view(batch_size,128,size_vocab)
            # print(data.shape)
            targets=torch.tensor(targets_lst,dtype=torch.float).view(batch_size,128,size_vocab)
            # print(targets.shape)
            # print(data[1])
            ## The above function has mapping related to the whole data and size vocab is added as it is the
            # length of unique characsters and our data might have any of them
            data=data.to(device)
            targets=targets.to(device)
            # targets = targets.view(-1, size_vocab)
            # print(data[0])

            model.zero_grad()

            data=torch.permute(data,(1,0,2))
            targets=torch.permute(targets,(1,0,2))
            output = model(data)
            output = output.view(-1, size_vocab)
            _,pred=torch.max(targets,2)
            labels=pred.view(-1)


            _, labels = torch.max(targets, 2)
            # print(labels.shape)
            labels=labels.view(-1)
            loss = criterion(output.view(-1, size_vocab), labels.view(-1))
            # loss = criterion(output, labels)

            loss.backward()

            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)



            running_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            optimizer.step()
            weight_updates+=1
            if (batch+1) % 32 == 0:
              print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, batch+1, total_step, loss.item()))
            if plot:
              info = { ('loss_' + model_name): loss.item() }

            ## Considering the validation error for future 32 characters after every 1000 weight updates
            if weight_updates%100 == 0:
                model.eval()
                running_loss_error_test=0
                test_total=0
                test_correct=0
                # if model_name != 'Transformer':
                #     hidden_op = model.init_hidden(batch_size)
                with torch.no_grad():
                    correct_val = 0
                    total_val = 0
                    for i in range(0, test_batch_data.shape[0]):
                        test_data, test_target = test_batch_data[i],test_batch_op[i]

                        #Performing one-hot encoding
                        test_data_lst,test_target_lst = [],[]
                        for x in test_data:
                          one=one_hot_encode(x,mapping,size_vocab)
                          test_data_lst.append(one)

                        for y in test_target:
                          one_tgt = one_hot_encode(y,mapping,size_vocab)
                          test_target_lst.append(one_tgt)

                        # batch_size = 16

                        test_data=torch.tensor(test_data_lst, dtype=torch.float).view(batch_size,128,size_vocab)
                        test_target=torch.tensor(test_target_lst,dtype=torch.float).view(batch_size,128,size_vocab)

                        # test_data=torch.tensor(one_hot_encode(test_data,mapping,size_vocab),dtype=torch.float)
                        # test_target=torch.tensor(one_hot_encode(test_target,mapping,size_vocab),dtype=torch.float)

                        test_data=test_data.to(device)
                        test_target=test_target.to(device)
                        # test_target = test_target.view(-1, size_vocab)

                        test_data=torch.permute(test_data,(1,0,2))
                        test_target=torch.permute(test_target,(1,0,2))
                        test_op = model(test_data)
                        test_op = test_op.view(-1, size_vocab)

                        _, test_labels = torch.max(test_target, 2)
                        test_labels=test_labels.view(-1)

                        loss_test = criterion(test_op, test_labels)
                        running_loss_error_test += (test_data.shape[0])*loss_test.item()
                        _, test_predicted = torch.max(test_op.data, 1)
                        test_total += test_labels.size(0)
                        test_correct += (test_predicted == test_labels).sum().item()
                model.train()
                print('Accuracy of the network on the test data: {} %'.format(100 * test_correct / test_total))
                val_error.append(100-100*test_correct/test_total)
                val_loss_values.append(running_loss_error_test/((test_batch_data.shape[0])*batch_size))

        train_loss_values.append(running_loss/(batch_data.shape[0]))
        train_error.append(100-100*correct/total)

    return val_error,val_loss_values,train_error,train_loss_values



ntokens = size_vocab
nhid = 400
lr = 0.001
nlayers = 1
nhead=2
nlayers=4
clip = 1
dropout = 0.0
ninp = size_vocab
model = TransformerModel(ntokens, ninp, nhead,nhid, nlayers, dropout).to(device)
criterion = nn.NLLLoss()
epochs = 5
optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, amsgrad=False)
val_error,val_loss_values,train_error,train_loss_values= train(model, optimizer, criterion, batch_data_trans, batch_op_trans, test_batch_data_trans, test_batch_op_trans, 256, size_vocab, mapping, epochs,'Trans curve','Transformer')
torch.save(model.state_dict(), "model_params_trans.pth")

"""## **PLOTS - Transformers**"""

plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(train_loss_values)
plt.title('Training Loss')
plt.xlabel('Number of Epochs')
plt.ylabel('Loss')

# Plotting the training error.
# plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 2)
val_loss_values_plt = [x/256 for x in val_loss_values]
plt.plot(val_loss_values_plt)
plt.title('Validation loss')
plt.xlabel('Number of Epochs')
plt.ylabel('Loss')

plt.tight_layout()
plt.show()


#%%

plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(train_error)
plt.title('Training Error')
plt.xlabel('Number of Epochs')
plt.ylabel('Error')

# Plotting the training error.
# plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 2)
plt.plot(val_error)
plt.title('Validation Error')
plt.xlabel('Number of Epochs')
plt.ylabel('Error')

plt.tight_layout()
plt.show()

"""Please ignore the code below. This is just to run the pretrained model run previously.

**Though do have a look at the sentence generation in the end.**
"""

ntokens = size_vocab
nhid = 400
lr = 0.001
nlayers = 1
nhead=2
nlayers=4
clip = 1
dropout = 0.0
ninp = size_vocab
model = TransformerModel(ntokens, ninp, nhead,nhid, nlayers, dropout).to(device)
criterion = nn.NLLLoss()
epochs = 10
optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, amsgrad=False)
# val_error,val_loss_values,train_error,train_loss_values= train(model, optimizer, criterion, batch_data_trans, batch_op_trans, test_batch_data_trans, test_batch_op_trans, 256, size_vocab, mapping, epochs,'Trans curve','Transformer')
# torch.save(model.state_dict(), "model_params_trans.pth")
model.load_state_dict(torch.load("model_params_trans.pth"))

"""### **Generation**"""

seq_length = 1024 #predicting from a starting sequence of length 16
# hid = model.init_hidden(1)
examples_seq = ["iNDIa is WINNing","how DO YOu Do bO","DL546 Hw3 iS eASy"]

for i in examples_seq:
    sentence = i
    for x in range(seq_length):
      data = np.array([char for char in i])
      data_one_hot = torch.tensor(one_hot_encode(data,mapping,size_vocab),dtype=torch.float).to(device)
      output = model(data_one_hot)
      output = output.view(-1, len(mapping))
      # print(output.shape)
      _, predicted = torch.max(output, dim=1)
      # print(predicted.shape)
      char_next = mapping_v2[predicted[-1].item()]
      sentence+=char_next
      data_one_hot = torch.tensor(one_hot_encode(sentence,mapping,size_vocab),dtype=torch.float).to(device)
    print(sentence)

